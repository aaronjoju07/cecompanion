{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG0x-afjDl6-"
      },
      "source": [
        "# PDF RAG Chatbot with LangChain and Gemini 1.5 Flash\n",
        "\n",
        "This notebook demonstrates how to build a RAG (Retrieval-Augmented Generation) system that can:\n",
        "1. Load and process PDF documents\n",
        "2. Create embeddings and store them in a vector database\n",
        "3. Retrieve relevant context based on user questions\n",
        "4. Generate accurate answers using Google's Gemini 1.5 Flash model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goLBnJscDl7A"
      },
      "source": [
        "## 1. Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDOZx9PPDl7A",
        "outputId": "0f7c4fc9-bb4c-437c-fb75-e29cec7df042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.0.11)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.3.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.43)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.6.16)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.8.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.24.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.25.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.69.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-google-genai langchain-community faiss-cpu pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twNbOf-GDl7B"
      },
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "d6HHltl8Dl7B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tempfile\n",
        "from IPython.display import display, Markdown\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2Wnfkk_Dl7B"
      },
      "source": [
        "## 3. Configure API Key\n",
        "\n",
        "Set your Google API key for accessing Gemini 1.5 Flash and embedding models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TObfPHtPDl7B"
      },
      "outputs": [],
      "source": [
        "# Set your Google API key here\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"KEY\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSbOPLJwDl7B"
      },
      "source": [
        "## 4. Define Functions for PDF Processing and RAG Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "S8rzln25Dl7C"
      },
      "outputs": [],
      "source": [
        "def process_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Process a PDF file and create a vector store from its content.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file\n",
        "\n",
        "    Returns:\n",
        "        FAISS vector store containing document chunks\n",
        "    \"\"\"\n",
        "    print(f\"Loading PDF from {pdf_path}...\")\n",
        "\n",
        "    # Load the PDF\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    print(f\"Loaded {len(documents)} pages from the PDF.\")\n",
        "\n",
        "    # Split text into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1500,\n",
        "        chunk_overlap=150\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "    print(f\"Split into {len(chunks)} chunks.\")\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    print(\"Creating embeddings and vector store...\")\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "    vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "    print(\"Vector store created successfully!\")\n",
        "    return vector_store\n",
        "\n",
        "def get_conversation_chain(vector_store):\n",
        "    \"\"\"\n",
        "    Create a conversational retrieval chain using the vector store.\n",
        "\n",
        "    Args:\n",
        "        vector_store: FAISS vector store containing document chunks\n",
        "\n",
        "    Returns:\n",
        "        ConversationalRetrievalChain for answering questions\n",
        "    \"\"\"\n",
        "    # Initialize the Gemini model\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-1.5-flash\",\n",
        "        temperature=0.2,\n",
        "        convert_system_message_to_human=True\n",
        "    )\n",
        "\n",
        "    # Initialize memory for conversation history\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        return_messages=True,\n",
        "        # Explicitly set the output key to 'answer'\n",
        "        output_key='answer'\n",
        "    )\n",
        "\n",
        "    # Create a conversational retrieval chain\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 5}),\n",
        "        memory=memory,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "    return conversation_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WGgirKwDl7C"
      },
      "source": [
        "## 5. Load and Process Your PDF\n",
        "\n",
        "Specify the path to your PDF file and process it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffR6m3_kDl7C",
        "outputId": "c894edb8-24a3-4f28-b119-5ac5f1261191"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading PDF from /content/event_demo .pdf...\n",
            "Loaded 3 pages from the PDF.\n",
            "Split into 3 chunks.\n",
            "Creating embeddings and vector store...\n",
            "Vector store created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Set the path to your PDF file\n",
        "pdf_path = \"/content/event_demo .pdf\"  # Replace with your actual PDF path\n",
        "\n",
        "# Process the PDF\n",
        "vector_store = process_pdf(pdf_path)\n",
        "\n",
        "# Create the conversation chain\n",
        "conversation_chain = get_conversation_chain(vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkoH9JVPDl7C"
      },
      "source": [
        "## 6. Chat Interface\n",
        "\n",
        "Ask questions about the PDF document and get responses from the RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2f2CO46TDl7C"
      },
      "outputs": [],
      "source": [
        "def ask_question(question):\n",
        "    \"\"\"\n",
        "    Ask a question about the PDF and display the response.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question to ask\n",
        "    \"\"\"\n",
        "    print(f\"\\nQuestion: {question}\")\n",
        "    print(\"\\nThinking...\")\n",
        "\n",
        "    # Get response from conversation chain\n",
        "    response = conversation_chain({\"question\": question})\n",
        "\n",
        "    # Extract the response text\n",
        "    answer = response[\"answer\"]\n",
        "\n",
        "    # Build formatted response with sources\n",
        "    formatted_response = f\"### Answer:\\n{answer}\\n\"\n",
        "\n",
        "    # Add citations if available\n",
        "    source_docs = response.get(\"source_documents\", [])\n",
        "    if source_docs:\n",
        "        formatted_response += \"\\n### Sources:\\n\"\n",
        "        for i, doc in enumerate(source_docs[:3]):  # Limit to top 3 sources\n",
        "            page_info = f\"Page {doc.metadata.get('page', 'unknown')}\"\n",
        "            formatted_response += f\"{i+1}. {page_info}\\n\"\n",
        "\n",
        "    # Display the formatted response\n",
        "    display(Markdown(formatted_response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS3dHr6VDl7D"
      },
      "source": [
        "## 7. Ask Questions About Your PDF\n",
        "\n",
        "Use the cell below to ask questions about the content of your PDF. You can run this cell multiple times with different questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "4pAtB35HDl7D",
        "outputId": "ac2e9ee2-00ab-4373-c177-67490c9c5d28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Question: What is the document about?\n",
            "\n",
            "Thinking...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:362: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### Answer:\n",
              "The document is about the Computer Science Fest 2025,  an event hosted by the Department of Computer Science at TechVille University.  It details the event's schedule, activities (including a hackathon, coding competitions, paper presentations, workshops, keynote speeches, and networking sessions), rules, prizes, registration process, and contact information.\n",
              "\n",
              "### Sources:\n",
              "1. Page 1\n",
              "2. Page 2\n",
              "3. Page 0\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "question = \"What is the document about?\"  # Replace with your question\n",
        "ask_question(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iZevnMrDl7D"
      },
      "source": [
        "## 8. Follow-up Questions\n",
        "\n",
        "The system maintains conversation history, so you can ask follow-up questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "o5UW4j3qDl7D",
        "outputId": "a8e9ced0-6cd9-4d2a-a5bf-03c503314233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Question: Can you provide more details about the first topic?\n",
            "\n",
            "Thinking...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:362: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:362: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### Answer:\n",
              "The Computer Science Fest 2025 will be held at TechVille University Auditorium, Block A, from April 15-17, 2025, 9:00 AM to 6:00 PM.  It's organized by the Department of Computer Science at TechVille University.  The event includes keynote speeches, technical workshops (requiring pre-registration and with limited seats), a 24-hour hackathon (teams of up to 4 members), coding competitions (individual, various skill levels, using Python, Java, C++, or JavaScript), paper presentations (computer science related topics, submission deadline April 10th), tech exhibitions, panel discussions, and networking sessions.  Participation certificates will be given to all registered participants.  There are prizes for the hackathon ($1000 for the winner, $500 for the runner-up), coding competition ($500/$250), and best paper presentation ($300/$150).  Registration is online at www.techvillecsfest.com by April 5, 2025.  Contact information is csfest@techville.edu or +1 (555) 123-4567.\n",
              "\n",
              "### Sources:\n",
              "1. Page 0\n",
              "2. Page 2\n",
              "3. Page 1\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "follow_up_question = \"Can you provide more details about the first topic?\"  # Replace with your follow-up\n",
        "ask_question(follow_up_question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZQiX5iNDl7D"
      },
      "source": [
        "## 9. Save Vector Store (Optional)\n",
        "\n",
        "You can save the vector store to disk for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAPGeY4-Dl7D",
        "outputId": "4299e5b5-182e-4bea-e08e-4bc7ba4402c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store saved to 'faiss_index' directory.\n"
          ]
        }
      ],
      "source": [
        "# Save the vector store to disk\n",
        "vector_store.save_local(\"faiss_index\")\n",
        "print(\"Vector store saved to 'faiss_index' directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3A7Ek8LDl7D"
      },
      "source": [
        "## 10. Load Vector Store (Optional)\n",
        "\n",
        "You can load a previously saved vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYJImN7nDl7D",
        "outputId": "120d9dee-0693-4edc-9073-985a7e483349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Load the vector store from disk\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "# Add allow_dangerous_deserialization=True to the load_local call\n",
        "loaded_vector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "loaded_conversation_chain = get_conversation_chain(loaded_vector_store)\n",
        "print(\"Vector store loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIXFhIIjOpT2"
      },
      "outputs": [],
      "source": [
        "# prompt: what to do after saving and storing vector store also explain why we do need vector store\n",
        "\n",
        "# ## 11.  Further actions after saving the vector store\n",
        "\n",
        "# After saving the vector store, you have several options depending on your needs:\n",
        "\n",
        "# 1. Deployment for Production Use:\n",
        "#    - Package the saved vector store (\"faiss_index\" directory in this case) along with your application code.\n",
        "#    - Use a production-ready vector database (like Weaviate, Pinecone, or Chroma) instead of FAISS for better performance and scalability in a real-world setting.  FAISS is great for experimentation and development but may not be suitable for high-traffic applications.\n",
        "#    - Deploy your application to a cloud platform (e.g., Google Cloud, AWS, Azure) or a server.  This will allow users to interact with the chatbot.\n",
        "#    - Set up API endpoints to handle incoming questions and return answers from your chatbot.\n",
        "\n",
        "# 2. Updating the Vector Store:\n",
        "#    - If new PDF documents are added or existing documents are updated, you need to re-process the documents and update the vector store.  You would load the existing vector store (as demonstrated in step 10), then add the new document embeddings using `vector_store.add_documents()` or similar functions.  After adding the new documents, save the updated vector store again.\n",
        "\n",
        "# 3.  Offline Usage:\n",
        "#    - You can use the saved vector store without an internet connection.  This is useful for environments without constant network access.\n",
        "\n",
        "# Why do we need a vector store?\n",
        "\n",
        "# A vector store is crucial for efficient similarity search. Here's why:\n",
        "\n",
        "# - Similarity Search:  The core idea is to convert text into vector embeddings (numerical representations).  When a user asks a question, the question is also converted to a vector. The vector store then quickly finds the most similar document chunks (those with vector representations closest to the question's vector) from the PDF. This is *much* faster than searching through all the text of the PDF directly.\n",
        "\n",
        "# - Contextual Relevance: By retrieving similar document chunks, you provide relevant context to the language model (like Gemini).  This context helps the model generate accurate and informative answers related to the user's question. Without the vector store, the LLM would only have access to its pre-trained knowledge, which might not be specific enough for a question about a particular PDF.\n",
        "\n",
        "# - Scalability: Vector stores enable efficient searches across large datasets.  Imagine a PDF that is hundreds or thousands of pages long; finding relevant text without a vector store would be extremely slow.\n",
        "\n",
        "# - Speed: Vector similarity search is very fast, especially for large datasets, because it involves comparing vectors mathematically (finding distances) rather than doing full-text string matching.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
