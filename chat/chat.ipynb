{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG0x-afjDl6-"
      },
      "source": [
        "# PDF RAG Chatbot with LangChain and Gemini 1.5 Flash\n",
        "\n",
        "This notebook demonstrates how to build a RAG (Retrieval-Augmented Generation) system that can:\n",
        "1. Load and process PDF documents\n",
        "2. Create embeddings and store them in a vector database\n",
        "3. Retrieve relevant context based on user questions\n",
        "4. Generate accurate answers using Google's Gemini 1.5 Flash model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goLBnJscDl7A"
      },
      "source": [
        "## 1. Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDOZx9PPDl7A",
        "outputId": "0f7c4fc9-bb4c-437c-fb75-e29cec7df042"
      },
      "outputs": [],
      "source": [
        "# pip3 install langchain langchain-google-genai langchain-community faiss-cpu pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twNbOf-GDl7B"
      },
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d6HHltl8Dl7B"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aaronjoju/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/aaronjoju/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tempfile\n",
        "from IPython.display import display, Markdown\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2Wnfkk_Dl7B"
      },
      "source": [
        "## 3. Configure API Key\n",
        "\n",
        "Set your Google API key for accessing Gemini 1.5 Flash and embedding models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TObfPHtPDl7B"
      },
      "outputs": [],
      "source": [
        "# Set your Google API key here\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCQm0KhDrntN2VaiTi7aHCoVpZYFMQo_jg\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSbOPLJwDl7B"
      },
      "source": [
        "## 4. Define Functions for PDF Processing and RAG Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "S8rzln25Dl7C"
      },
      "outputs": [],
      "source": [
        "def process_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Process a PDF file and create a vector store from its content.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file\n",
        "\n",
        "    Returns:\n",
        "        FAISS vector store containing document chunks\n",
        "    \"\"\"\n",
        "    print(f\"Loading PDF from {pdf_path}...\")\n",
        "\n",
        "    # Load the PDF\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    print(f\"Loaded {len(documents)} pages from the PDF.\")\n",
        "\n",
        "    # Split text into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1500,\n",
        "        chunk_overlap=150\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "    print(f\"Split into {len(chunks)} chunks.\")\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    print(\"Creating embeddings and vector store...\")\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "    vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "    print(\"Vector store created successfully!\")\n",
        "    return vector_store\n",
        "\n",
        "def get_conversation_chain(vector_store):\n",
        "    \"\"\"\n",
        "    Create a conversational retrieval chain using the vector store.\n",
        "\n",
        "    Args:\n",
        "        vector_store: FAISS vector store containing document chunks\n",
        "\n",
        "    Returns:\n",
        "        ConversationalRetrievalChain for answering questions\n",
        "    \"\"\"\n",
        "    # Initialize the Gemini model\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-1.5-flash\",\n",
        "        temperature=0.2,\n",
        "        convert_system_message_to_human=True\n",
        "    )\n",
        "\n",
        "    # Initialize memory for conversation history\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        return_messages=True,\n",
        "        # Explicitly set the output key to 'answer'\n",
        "        output_key='answer'\n",
        "    )\n",
        "\n",
        "    # Create a conversational retrieval chain\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 5}),\n",
        "        memory=memory,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "    return conversation_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WGgirKwDl7C"
      },
      "source": [
        "## 5. Load and Process Your PDF\n",
        "\n",
        "Specify the path to your PDF file and process it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffR6m3_kDl7C",
        "outputId": "c894edb8-24a3-4f28-b119-5ac5f1261191"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading PDF from /Users/aaronjoju/Downloads/event_demo .pdf...\n",
            "Loaded 3 pages from the PDF.\n",
            "Split into 3 chunks.\n",
            "Creating embeddings and vector store...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1743601445.108830 8291381 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store created successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/p5/ck_2rv1n3rn6dwp8yldw0z7w0000gn/T/ipykernel_3212/417919514.py:54: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(\n",
            "I0000 00:00:1743601452.177432 8291381 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
          ]
        }
      ],
      "source": [
        "# Set the path to your PDF file\n",
        "pdf_path = \"/Users/aaronjoju/Downloads/event_demo .pdf\"  # Replace with your actual PDF path\n",
        "\n",
        "# Process the PDF\n",
        "vector_store = process_pdf(pdf_path)\n",
        "\n",
        "# Create the conversation chain\n",
        "conversation_chain = get_conversation_chain(vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkoH9JVPDl7C"
      },
      "source": [
        "## 6. Chat Interface\n",
        "\n",
        "Ask questions about the PDF document and get responses from the RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2f2CO46TDl7C"
      },
      "outputs": [],
      "source": [
        "def ask_question(question):\n",
        "    \"\"\"\n",
        "    Ask a question about the PDF and display the response.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question to ask\n",
        "    \"\"\"\n",
        "    print(f\"\\nQuestion: {question}\")\n",
        "    print(\"\\nThinking...\")\n",
        "\n",
        "    # Get response from conversation chain\n",
        "    response = conversation_chain({\"question\": question})\n",
        "\n",
        "    # Extract the response text\n",
        "    answer = response[\"answer\"]\n",
        "\n",
        "    # Build formatted response with sources\n",
        "    formatted_response = f\"### Answer:\\n{answer}\\n\"\n",
        "\n",
        "    # Add citations if available\n",
        "    source_docs = response.get(\"source_documents\", [])\n",
        "    if source_docs:\n",
        "        formatted_response += \"\\n### Sources:\\n\"\n",
        "        for i, doc in enumerate(source_docs[:3]):  # Limit to top 3 sources\n",
        "            page_info = f\"Page {doc.metadata.get('page', 'unknown')}\"\n",
        "            formatted_response += f\"{i+1}. {page_info}\\n\"\n",
        "\n",
        "    # Display the formatted response\n",
        "    display(Markdown(formatted_response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS3dHr6VDl7D"
      },
      "source": [
        "## 7. Ask Questions About Your PDF\n",
        "\n",
        "Use the cell below to ask questions about the content of your PDF. You can run this cell multiple times with different questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "4pAtB35HDl7D",
        "outputId": "ac2e9ee2-00ab-4373-c177-67490c9c5d28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Question: What is the document about?\n",
            "\n",
            "Thinking...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/p5/ck_2rv1n3rn6dwp8yldw0z7w0000gn/T/ipykernel_3212/556807086.py:12: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = conversation_chain({\"question\": question})\n",
            "/Users/aaronjoju/Library/Python/3.9/lib/python/site-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### Answer:\n",
              "The document is about the Computer Science Fest 2025,  an event hosted by the Department of Computer Science at TechVille University.  It details the event's schedule, activities (including a hackathon, coding competitions, paper presentations, workshops, keynote speeches, and networking sessions), rules, prizes, registration process, and contact information.\n",
              "\n",
              "### Sources:\n",
              "1. Page 1\n",
              "2. Page 2\n",
              "3. Page 0\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "question = \"What is the document about?\"  # Replace with your question\n",
        "ask_question(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iZevnMrDl7D"
      },
      "source": [
        "## 8. Follow-up Questions\n",
        "\n",
        "The system maintains conversation history, so you can ask follow-up questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "o5UW4j3qDl7D",
        "outputId": "a8e9ced0-6cd9-4d2a-a5bf-03c503314233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Question: Can you provide more details about the first topic?\n",
            "\n",
            "Thinking...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aaronjoju/Library/Python/3.9/lib/python/site-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/Users/aaronjoju/Library/Python/3.9/lib/python/site-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### Answer:\n",
              "The Computer Science Fest 2025 will be held at TechVille University Auditorium, Block A, from April 15-17, 2025, 9:00 AM to 6:00 PM.  It's organized by the Department of Computer Science at TechVille University.  The event includes keynote speeches, technical workshops (requiring pre-registration and with limited seats), a 24-hour hackathon (teams of up to 4 members), coding competitions (individual, various skill levels, using Python, Java, C++, or JavaScript), paper presentations (with a submission deadline of April 10th), tech exhibitions, panel discussions, and networking sessions.  Registration closes April 5th, 2025, and can be done online at www.techvillecsfest.com.  Prizes are awarded for the hackathon, coding competition, and best paper presentation.  Participation certificates will be given to all registered participants.  Contact information is: csfest@techville.edu or +1 (555) 123-4567.\n",
              "\n",
              "### Sources:\n",
              "1. Page 0\n",
              "2. Page 2\n",
              "3. Page 1\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "follow_up_question = \"Can you provide more details about the first topic?\"  # Replace with your follow-up\n",
        "ask_question(follow_up_question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZQiX5iNDl7D"
      },
      "source": [
        "## 9. Save Vector Store (Optional)\n",
        "\n",
        "You can save the vector store to disk for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAPGeY4-Dl7D",
        "outputId": "4299e5b5-182e-4bea-e08e-4bc7ba4402c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store saved to 'faiss_index' directory.\n"
          ]
        }
      ],
      "source": [
        "# Save the vector store to disk\n",
        "vector_store.save_local(\"faiss_index\")\n",
        "print(\"Vector store saved to 'faiss_index' directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3A7Ek8LDl7D"
      },
      "source": [
        "## 10. Load Vector Store (Optional)\n",
        "\n",
        "You can load a previously saved vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYJImN7nDl7D",
        "outputId": "120d9dee-0693-4edc-9073-985a7e483349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store loaded successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1743601463.247500 8291381 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
            "I0000 00:00:1743601463.249200 8291381 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
          ]
        }
      ],
      "source": [
        "# Load the vector store from disk\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "# Add allow_dangerous_deserialization=True to the load_local call\n",
        "loaded_vector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "loaded_conversation_chain = get_conversation_chain(loaded_vector_store)\n",
        "print(\"Vector store loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aaronjoju/Library/Python/3.9/lib/python/site-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: The main topic of the document is the Computer Science Fest 2025, including details about its events, registration, prizes, and contact information.\n",
            "\n",
            "Sources:\n",
            "1. Page 1\n",
            "2. Page 2\n",
            "3. Page 0\n"
          ]
        }
      ],
      "source": [
        "# Ask a question using the loaded vector store\n",
        "question = \"What is the main topic of the document?\"  # Replace with your question\n",
        "response = loaded_conversation_chain({\"question\": question})\n",
        "\n",
        "# Extract and display the answer\n",
        "answer = response[\"answer\"]\n",
        "print(f\"Answer: {answer}\")\n",
        "\n",
        "# Display sources if available\n",
        "source_docs = response.get(\"source_documents\", [])\n",
        "if source_docs:\n",
        "    print(\"\\nSources:\")\n",
        "    for i, doc in enumerate(source_docs[:3]):  # Limit to top 3 sources\n",
        "        page_info = f\"Page {doc.metadata.get('page', 'unknown')}\"\n",
        "        print(f\"{i+1}. {page_info}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to the new PDF file\n",
        "new_pdf_path = \"/path/to/your/new_pdf.pdf\"  # Replace with the actual path to the new PDF\n",
        "\n",
        "# Process the new PDF to create its vector store\n",
        "new_vector_store = process_pdf(new_pdf_path)\n",
        "\n",
        "# Merge the new vector store with the existing one\n",
        "vector_store.merge_from(new_vector_store)\n",
        "\n",
        "print(\"New PDF added successfully. The vector store now contains data from both PDFs.\")\n",
        "\n",
        "# Save the updated vector store to disk (optional)\n",
        "vector_store.save_local(\"faiss_index_combined\")\n",
        "print(\"Updated vector store saved to 'faiss_index_combined' directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload the vector store for the second PDF (or other valid PDFs)\n",
        "new_vector_store = FAISS.load_local(\"faiss_index_combined\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Filter out the embeddings related to the first PDF\n",
        "# Assuming you have metadata to identify the source of each document\n",
        "valid_documents = [\n",
        "    doc for doc in new_vector_store.docstore.values()\n",
        "    if doc.metadata.get(\"source\") != pdf_path  # Exclude documents from the first PDF\n",
        "]\n",
        "\n",
        "# Rebuild the vector store with only valid documents\n",
        "filtered_vector_store = FAISS.from_documents(valid_documents, embeddings)\n",
        "\n",
        "# Save the updated vector store\n",
        "filtered_vector_store.save_local(\"faiss_index_filtered\")\n",
        "print(\"Filtered vector store saved to 'faiss_index_filtered' directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIXFhIIjOpT2"
      },
      "outputs": [],
      "source": [
        "# prompt: what to do after saving and storing vector store also explain why we do need vector store\n",
        "\n",
        "# ## 11.  Further actions after saving the vector store\n",
        "\n",
        "# After saving the vector store, you have several options depending on your needs:\n",
        "\n",
        "# 1. Deployment for Production Use:\n",
        "#    - Package the saved vector store (\"faiss_index\" directory in this case) along with your application code.\n",
        "#    - Use a production-ready vector database (like Weaviate, Pinecone, or Chroma) instead of FAISS for better performance and scalability in a real-world setting.  FAISS is great for experimentation and development but may not be suitable for high-traffic applications.\n",
        "#    - Deploy your application to a cloud platform (e.g., Google Cloud, AWS, Azure) or a server.  This will allow users to interact with the chatbot.\n",
        "#    - Set up API endpoints to handle incoming questions and return answers from your chatbot.\n",
        "\n",
        "# 2. Updating the Vector Store:\n",
        "#    - If new PDF documents are added or existing documents are updated, you need to re-process the documents and update the vector store.  You would load the existing vector store (as demonstrated in step 10), then add the new document embeddings using `vector_store.add_documents()` or similar functions.  After adding the new documents, save the updated vector store again.\n",
        "\n",
        "# 3.  Offline Usage:\n",
        "#    - You can use the saved vector store without an internet connection.  This is useful for environments without constant network access.\n",
        "\n",
        "# Why do we need a vector store?\n",
        "\n",
        "# A vector store is crucial for efficient similarity search. Here's why:\n",
        "\n",
        "# - Similarity Search:  The core idea is to convert text into vector embeddings (numerical representations).  When a user asks a question, the question is also converted to a vector. The vector store then quickly finds the most similar document chunks (those with vector representations closest to the question's vector) from the PDF. This is *much* faster than searching through all the text of the PDF directly.\n",
        "\n",
        "# - Contextual Relevance: By retrieving similar document chunks, you provide relevant context to the language model (like Gemini).  This context helps the model generate accurate and informative answers related to the user's question. Without the vector store, the LLM would only have access to its pre-trained knowledge, which might not be specific enough for a question about a particular PDF.\n",
        "\n",
        "# - Scalability: Vector stores enable efficient searches across large datasets.  Imagine a PDF that is hundreds or thousands of pages long; finding relevant text without a vector store would be extremely slow.\n",
        "\n",
        "# - Speed: Vector similarity search is very fast, especially for large datasets, because it involves comparing vectors mathematically (finding distances) rather than doing full-text string matching.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating embeddings and vector store...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1743605424.558713 8291381 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store created successfully!\n",
            "\n",
            "Question: What is the event description?\n",
            "\n",
            "Thinking...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1743605431.336996 8291381 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
            "/Users/aaronjoju/Library/Python/3.9/lib/python/site-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### Answer:\n",
              "Annual technology festival showcasing student innovations.\n",
              "\n",
              "### Sources:\n",
              "1. Page unknown\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import json\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "def process_json(json_data):\n",
        "    \"\"\"\n",
        "    Process JSON data and create a vector store from its content.\n",
        "\n",
        "    Args:\n",
        "        json_data (dict): JSON data to process\n",
        "\n",
        "    Returns:\n",
        "        FAISS vector store containing document chunks\n",
        "    \"\"\"\n",
        "    # Convert JSON data to a string representation\n",
        "    json_text = json.dumps(json_data, indent=2)\n",
        "\n",
        "    # Split text into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1500,\n",
        "        chunk_overlap=150\n",
        "    )\n",
        "    chunks = text_splitter.split_text(json_text)\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    print(\"Creating embeddings and vector store...\")\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "    vector_store = FAISS.from_texts(chunks, embeddings)\n",
        "\n",
        "    print(\"Vector store created successfully!\")\n",
        "    return vector_store\n",
        "\n",
        "# Example JSON data\n",
        "json_data = {\n",
        "    \"name\": \"Tech Fest 2025\",\n",
        "    \"description\": \"Annual technology festival showcasing student innovations.\",\n",
        "    \"conductedDates\": {\n",
        "        \"start\": \"2025-04-15T09:00:00Z\",\n",
        "        \"end\": \"2025-04-17T17:00:00Z\"\n",
        "    },\n",
        "    \"targetedAudience\": {\n",
        "        \"departments\": [\"Computer Science\", \"Electrical Engineering\"],\n",
        "        \"courses\": [\"B.Tech\", \"M.Tech\"]\n",
        "    },\n",
        "    \"organizingInstitution\": \"XYZ University\",\n",
        "    \"maximumStudents\": 200,\n",
        "    \"maxEventsPerStudent\": 3,\n",
        "    \"organizingCollege\": \"ABC College of Engineering\",\n",
        "    \"generalRules\": [\"No outside food\", \"ID required for entry\"],\n",
        "    \"contactInfo\": {\n",
        "        \"email\": \"events@xyz.edu\",\n",
        "        \"phone\": \"+1234567890\"\n",
        "    },\n",
        "    \"subEvents\": [\n",
        "        {\n",
        "            \"name\": \"Hackathon\",\n",
        "            \"overview\": \"24-hour coding challenge\",\n",
        "            \"venue\": \"Main Hall\",\n",
        "            \"prizePools\": [{ \"rank\": 1, \"amount\": 1000 }]\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Robotics Workshop\",\n",
        "            \"overview\": \"Hands-on robotics session\",\n",
        "            \"venue\": \"Lab 3\",\n",
        "            \"prizePools\": []\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Process the JSON data\n",
        "vector_store = process_json(json_data)\n",
        "\n",
        "# You can now use the vector store to create a conversational chain or perform similarity searches\n",
        "conversation_chain = get_conversation_chain(vector_store)\n",
        "\n",
        "# Example question\n",
        "question = \"What is the event description?\"\n",
        "ask_question(question)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
